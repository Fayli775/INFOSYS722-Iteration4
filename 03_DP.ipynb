{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwWmP9eDfPBJZJBbXLEVhx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fayli775/INFOSYS722-Iteration4/blob/main/03_DP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLmSUoS7ZuPP",
        "outputId": "fd188bc5-0ea0-461f-a4c7-8dda3f2c5487"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[COUNT] raw -> 6660911\n",
            "root\n",
            " |-- OBJECTID: integer (nullable = true)\n",
            " |-- Start Date: string (nullable = true)\n",
            " |-- Site Alias: integer (nullable = true)\n",
            " |-- Region Name: string (nullable = true)\n",
            " |-- Site Reference: string (nullable = true)\n",
            " |-- Class Weight: string (nullable = true)\n",
            " |-- Site Description: string (nullable = true)\n",
            " |-- Lane Number: integer (nullable = true)\n",
            " |-- Flow Direction: integer (nullable = true)\n",
            " |-- Traffic Count: double (nullable = true)\n",
            "\n",
            "[COUNT] after selection and region filter -> 1922734\n",
            "[NULL REPORT] selection key fields rows=1922734 cols=5\n",
            "+----------+------------+-----------+--------------+-------------+\n",
            "|Start Date|Class Weight|Lane Number|Flow Direction|Traffic Count|\n",
            "+----------+------------+-----------+--------------+-------------+\n",
            "|0         |0           |0          |0             |0            |\n",
            "+----------+------------+-----------+--------------+-------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !pip install pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "from pyspark.sql.functions import col, when, lit, to_timestamp, to_date, date_format, year, month, dayofmonth, hour, dayofweek, format_string, regexp_replace\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "spark = (\n",
        "    SparkSession.builder\n",
        "    .appName(\"Iteration4-DataPreparation\")\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
        "    .getOrCreate()\n",
        ")\n",
        "\n",
        "traffic_fp = \"/content/drive/MyDrive/722/TMS_Telemetry_Sites.csv\"\n",
        "holidays_fp = \"/content/drive/MyDrive/722/NZ_Holidays.csv\"\n",
        "\n",
        "df_raw = spark.read.csv(traffic_fp, header=True, inferSchema=True)\n",
        "\n",
        "def null_report(df, title=\"\"):\n",
        "    nr = df.select([F.count(F.when(F.col(c).isNull() | F.isnan(c), c)).alias(c) for c in df.columns])\n",
        "    print(f\"[NULL REPORT] {title} rows={df.count()} cols={len(df.columns)}\")\n",
        "    nr.show(truncate=False)\n",
        "\n",
        "def count_log(df, tag):\n",
        "    cnt = df.count()\n",
        "    print(f\"[COUNT] {tag} -> {cnt}\")\n",
        "    return cnt\n",
        "\n",
        "# Work with original column names - no renaming\n",
        "df = df_raw\n",
        "\n",
        "count_log(df, \"raw\")\n",
        "df.printSchema()\n",
        "\n",
        "# Use original column names for processing\n",
        "target_col = \"Traffic Count\"\n",
        "\n",
        "# Drop columns using original names\n",
        "drop_text = [c for c in [\"Site Alias\", \"Site Description\"] if c in df.columns]\n",
        "drop_sys = [c for c in [\"OBJECTID\"] if c in df.columns]\n",
        "df = df.drop(*drop_text).drop(*drop_sys)\n",
        "\n",
        "# Cast target column to double\n",
        "if target_col in df.columns:\n",
        "    df = df.withColumn(target_col, col(target_col).cast(\"double\"))\n",
        "\n",
        "# Filter for Auckland using original column name\n",
        "if \"Region Name\" in df.columns:\n",
        "    df = df.filter(col(\"Region Name\").contains(\"Auckland\"))\n",
        "\n",
        "count_log(df, \"after selection and region filter\")\n",
        "null_report(df.select([c for c in df.columns if c in [\"Start Date\",\"Lane Number\",\"Flow Direction\",\"Class Weight\",target_col]]), \"selection key fields\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample of filtered data:\")\n",
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZF--Ynl_jpPA",
        "outputId": "ecb8c5cc-aed6-47c1-e306-e35c09e5b8dc"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of filtered data:\n",
            "+--------------------+-------------+--------------+------------+-----------+--------------+-------------+\n",
            "|Start Date          |Region Name  |Site Reference|Class Weight|Lane Number|Flow Direction|Traffic Count|\n",
            "+--------------------+-------------+--------------+------------+-----------+--------------+-------------+\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|01600023      |Light       |2          |2             |18585.0      |\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|01600023      |Heavy       |1          |1             |924.0        |\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|01600023      |Light       |1          |1             |18508.0      |\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|01600023      |Heavy       |2          |2             |930.0        |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|01600023      |Heavy       |2          |2             |938.0        |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|01600023      |Light       |1          |1             |18775.5      |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|01600023      |Light       |2          |2             |18914.0      |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|01600023      |Heavy       |1          |1             |943.5        |\n",
            "|4/7/2018 12:00:00 AM|02 - Auckland|01600023      |Heavy       |2          |2             |571.0        |\n",
            "|4/7/2018 12:00:00 AM|02 - Auckland|01600023      |Light       |2          |2             |17523.0      |\n",
            "+--------------------+-------------+--------------+------------+-----------+--------------+-------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if target_col in df.columns:\n",
        "    df = df.filter(col(target_col).isNotNull())\n",
        "\n",
        "if \"Lane Number\" in df.columns:\n",
        "    df = df.withColumn(\"Lane Number\", col(\"Lane Number\").cast(\"int\"))\n",
        "    df = df.withColumn(\n",
        "        \"lane_number_fixed\",\n",
        "        when(col(\"Lane Number\") < 1, lit(1)).when(col(\"Lane Number\") > 6, lit(6)).otherwise(col(\"Lane Number\"))\n",
        "    ).withColumn(\n",
        "        \"lane_number_out_of_range\",\n",
        "        when((col(\"Lane Number\") < 1) | (col(\"Lane Number\") > 6), lit(1)).otherwise(lit(0))\n",
        "    ).drop(\"Lane Number\").withColumnRenamed(\"lane_number_fixed\", \"Lane Number\")\n",
        "\n",
        "if target_col in df.columns:\n",
        "    try:\n",
        "        p999 = df.approxQuantile(target_col, [0.999], 0.001)[0]\n",
        "    except Exception:\n",
        "        p999 = 60000.0\n",
        "    soft_cap = float(p999 if p999 and p999 > 0 else 60000.0)\n",
        "    hard_cap = 200000.0\n",
        "    print(f\"[INFO] traffic_count soft_cap={soft_cap:.1f}, hard_cap={hard_cap:.1f}\")\n",
        "    df = df.withColumn(\"traffic_count_raw\", col(target_col)).withColumn(target_col, F.least(col(target_col), lit(soft_cap), lit(hard_cap)))\n",
        "\n",
        "if \"Site Reference\" in df.columns:\n",
        "    completeness = df.filter(col(\"Site Reference\").isNotNull()).count() / max(df.count(), 1)\n",
        "    print(f\"[INFO] site_reference completeness: {completeness:.4f}\")\n",
        "    df = df.drop(\"Site Reference\")\n",
        "\n",
        "# Enhanced final data quality assessment\n",
        "total_records = df.count()\n",
        "print(f\"Total records after cleaning: {total_records:,}\")\n",
        "\n",
        "# Check all key analytical fields\n",
        "key_fields = [\"Lane Number\", \"Flow Direction\", \"Class Weight\", target_col]\n",
        "available_fields = [c for c in key_fields if c in df.columns]\n",
        "\n",
        "print(f\"\\nAnalytical fields assessment:\")\n",
        "for field in available_fields:\n",
        "    null_count = df.filter(col(field).isNull()).count()\n",
        "    completeness = ((total_records - null_count) / total_records) * 100\n",
        "    print(f\"  {field}: {completeness:.2f}% complete ({null_count:,} nulls)\")\n",
        "\n",
        "# Overall null report for all key fields\n",
        "null_report(df.select(available_fields), \"FINAL CLEANED DATASET - Key Analytical Fields\")\n",
        "\n",
        "# Additional statistics\n",
        "print(f\"\\nDataset summary:\")\n",
        "print(f\"  - Total columns: {len(df.columns)}\")\n",
        "print(f\"  - Key analytical fields: {len(available_fields)}\")\n",
        "print(f\"  - Records retained: {total_records:,}\")\n",
        "\n",
        "# Lane number corrections summary (if applied)\n",
        "if \"lane_number_out_of_range\" in df.columns:\n",
        "    corrections = df.filter(col(\"lane_number_out_of_range\") == 1).count()\n",
        "    print(f\"\\nLane number corrections applied: {corrections:,} records\")\n",
        "    df = df.drop(\"lane_number_out_of_range\")  # Clean up audit column\n",
        "\n",
        "# Traffic count capping summary\n",
        "if \"traffic_count_raw\" in df.columns:\n",
        "    capped_records = df.filter(col(\"traffic_count_raw\") != col(target_col)).count()\n",
        "    print(f\"Traffic count values capped: {capped_records:,} records\")\n",
        "\n",
        "    if capped_records > 0:\n",
        "        print(\"Traffic count distribution after capping:\")\n",
        "        df.select(target_col).describe().show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWd5X8NMaD6v",
        "outputId": "113f26e6-efe4-47e9-87f6-3a124b4c43eb"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] traffic_count soft_cap=33898.5, hard_cap=200000.0\n",
            "[INFO] site_reference completeness: 1.0000\n",
            "Total records after cleaning: 1,922,734\n",
            "\n",
            "Analytical fields assessment:\n",
            "  Lane Number: 100.00% complete (0 nulls)\n",
            "  Flow Direction: 100.00% complete (0 nulls)\n",
            "  Class Weight: 100.00% complete (0 nulls)\n",
            "  Traffic Count: 100.00% complete (0 nulls)\n",
            "[NULL REPORT] FINAL CLEANED DATASET - Key Analytical Fields rows=1922734 cols=4\n",
            "+-----------+--------------+------------+-------------+\n",
            "|Lane Number|Flow Direction|Class Weight|Traffic Count|\n",
            "+-----------+--------------+------------+-------------+\n",
            "|0          |0             |0           |0            |\n",
            "+-----------+--------------+------------+-------------+\n",
            "\n",
            "\n",
            "Dataset summary:\n",
            "  - Total columns: 8\n",
            "  - Key analytical fields: 4\n",
            "  - Records retained: 1,922,734\n",
            "\n",
            "Lane number corrections applied: 0 records\n",
            "Traffic count values capped: 0 records\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample of data:\")\n",
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLS33kokmcP1",
        "outputId": "c77e64df-14c6-4758-f361-3ad63e062093"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of data:\n",
            "+--------------------+-------------+------------+--------------+-------------+-----------+-----------------+\n",
            "|Start Date          |Region Name  |Class Weight|Flow Direction|Traffic Count|Lane Number|traffic_count_raw|\n",
            "+--------------------+-------------+------------+--------------+-------------+-----------+-----------------+\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|Light       |2             |18585.0      |2          |18585.0          |\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|Heavy       |1             |924.0        |1          |924.0            |\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|Light       |1             |18508.0      |1          |18508.0          |\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|Heavy       |2             |930.0        |2          |930.0            |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|Heavy       |2             |938.0        |2          |938.0            |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|Light       |1             |18775.5      |1          |18775.5          |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|Light       |2             |18914.0      |2          |18914.0          |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|Heavy       |1             |943.5        |1          |943.5            |\n",
            "|4/7/2018 12:00:00 AM|02 - Auckland|Heavy       |2             |571.0        |2          |571.0            |\n",
            "|4/7/2018 12:00:00 AM|02 - Auckland|Light       |2             |17523.0      |2          |17523.0          |\n",
            "+--------------------+-------------+------------+--------------+-------------+-----------+-----------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3 Data Construction\n",
        "date_col = \"Start Date\"\n",
        "if date_col in df.columns:\n",
        "    df = df.withColumn(\"StartDate_parsed\", F.to_timestamp(F.col(date_col), \"M/d/yyyy h:mm:ss a\")) \\\n",
        "           .withColumn(\"year\", F.year(\"StartDate_parsed\")) \\\n",
        "           .withColumn(\"month\", F.month(\"StartDate_parsed\")) \\\n",
        "           .withColumn(\"day\", F.dayofmonth(\"StartDate_parsed\")) \\\n",
        "           .withColumn(\"hour\", F.hour(\"StartDate_parsed\")) \\\n",
        "           .withColumn(\"dow_num\", F.dayofweek(\"StartDate_parsed\")) \\\n",
        "           .withColumn(\"weekday\", F.date_format(\"StartDate_parsed\", \"E\"))\n",
        "\n",
        "    # Check parsing success\n",
        "    parsing_success = df.filter(col(\"StartDate_parsed\").isNotNull()).count()\n",
        "    total_rows = df.count()\n",
        "    print(f\"[INFO] Date parsing success: {parsing_success}/{total_rows} ({parsing_success/total_rows*100:.1f}%)\")\n",
        "\n",
        "if \"Flow Direction\" in df.columns:\n",
        "    df = df.withColumn(\"Flow Direction\", col(\"Flow Direction\").cast(\"int\"))\n",
        "    df = df.withColumn(\n",
        "        \"flow_direction_grp\",\n",
        "        when(col(\"Flow Direction\").isin(1, 2), col(\"Flow Direction\").cast(\"string\"))\n",
        "        .when(col(\"Flow Direction\").isin(3, 4, 5, 6), lit(\"Other\"))\n",
        "        .otherwise(lit(\"Other\"))\n",
        "    ).drop(\"Flow Direction\").withColumnRenamed(\"flow_direction_grp\", \"Flow Direction\")\n",
        "\n",
        "if \"Class Weight\" in df.columns:\n",
        "    df = df.withColumn(\"Class Weight\", F.coalesce(col(\"Class Weight\").cast(\"string\"), lit(\"Unknown\")))\n",
        "\n",
        "# Update the essential columns list to match the new column names\n",
        "essential_cols = [c for c in [\"year\",\"month\",\"day\",\"hour\",\"weekday\",\"Lane Number\",\"Flow Direction\",\"Class Weight\",target_col] if c in df.columns]\n",
        "null_report(df.select(*essential_cols), \"after construction essential\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuAXNfvoaHZC",
        "outputId": "3e30f046-cc17-445d-81df-b5e0f8ca96ae"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Date parsing success: 1922734/1922734 (100.0%)\n",
            "[NULL REPORT] after construction essential rows=1922734 cols=9\n",
            "+----+-----+---+----+-------+-----------+--------------+------------+-------------+\n",
            "|year|month|day|hour|weekday|Lane Number|Flow Direction|Class Weight|Traffic Count|\n",
            "+----+-----+---+----+-------+-----------+--------------+------------+-------------+\n",
            "|0   |0    |0  |0   |0      |0          |0             |0           |0            |\n",
            "+----+-----+---+----+-------+-----------+--------------+------------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.3.5 Feature Construction Validation\n",
        "\n",
        "# Check all essential constructed features\n",
        "essential_cols = [c for c in [\"year\",\"month\",\"day\",\"hour\",\"weekday\",\"Lane Number\",\"Flow Direction\",\"Class Weight\",target_col] if c in df.columns]\n",
        "null_report(df.select(*essential_cols), \"after construction essential\")\n",
        "\n",
        "# Temporal feature range validation\n",
        "print(\"\\nTemporal feature ranges:\")\n",
        "if \"year\" in df.columns:\n",
        "    year_range = df.select(F.min(\"year\"), F.max(\"year\")).collect()[0]\n",
        "    print(f\"  Year range: {year_range[0]} to {year_range[1]}\")\n",
        "\n",
        "if \"month\" in df.columns:\n",
        "    month_range = df.select(F.min(\"month\"), F.max(\"month\")).collect()[0]\n",
        "    print(f\"  Month range: {month_range[0]} to {month_range[1]}\")\n",
        "\n",
        "if \"hour\" in df.columns:\n",
        "    hour_stats = df.select(\"hour\").distinct().orderBy(\"hour\").collect()\n",
        "    hours = [row[0] for row in hour_stats]\n",
        "    print(f\"  Hour values: {hours}\")\n",
        "\n",
        "# Flow Direction distribution after grouping\n",
        "print(\"\\nFlow Direction category distribution:\")\n",
        "if \"Flow Direction\" in df.columns:\n",
        "    flow_dist = df.groupBy(\"Flow Direction\").count().orderBy(F.desc(\"count\"))\n",
        "    flow_dist.show()\n",
        "\n",
        "# Class Weight distribution\n",
        "print(\"Class Weight distribution:\")\n",
        "if \"Class Weight\" in df.columns:\n",
        "    class_dist = df.groupBy(\"Class Weight\").count().orderBy(F.desc(\"count\"))\n",
        "    class_dist.show()\n",
        "\n",
        "# Weekday distribution validation\n",
        "print(\"Weekday distribution:\")\n",
        "if \"weekday\" in df.columns:\n",
        "    weekday_dist = df.groupBy(\"weekday\").count().orderBy(\"weekday\")\n",
        "    weekday_dist.show()\n",
        "\n",
        "# Sample of constructed features\n",
        "print(\"\\nSample of constructed data with new features:\")\n",
        "sample_cols = [\"Start Date\"] + essential_cols\n",
        "available_sample_cols = [c for c in sample_cols if c in df.columns]\n",
        "df.select(*available_sample_cols).show(5, truncate=False)\n",
        "\n",
        "# Construction impact summary\n",
        "total_original_cols = len([c for c in df.columns if not c.startswith(\"year\") and not c.startswith(\"month\") and not c.startswith(\"day\") and not c.startswith(\"hour\") and not c.startswith(\"weekday\") and not c.startswith(\"dow_num\") and c != \"StartDate_parsed\"])\n",
        "total_current_cols = len(df.columns)\n",
        "constructed_features = len([c for c in [\"year\",\"month\",\"day\",\"hour\",\"weekday\",\"dow_num\",\"StartDate_parsed\"] if c in df.columns])\n",
        "\n",
        "print(f\"\\nConstruction Summary:\")\n",
        "print(f\"  - Original analytical features: {len(essential_cols) - constructed_features}\")\n",
        "print(f\"  - Newly constructed features: {constructed_features}\")\n",
        "print(f\"  - Total analytical features: {len(essential_cols)}\")\n",
        "print(f\"  - Records maintained: {df.count():,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuaIfc3mIEdR",
        "outputId": "a97544c9-ac55-4886-fef8-15f09064ba76"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NULL REPORT] after construction essential rows=1922734 cols=9\n",
            "+----+-----+---+----+-------+-----------+--------------+------------+-------------+\n",
            "|year|month|day|hour|weekday|Lane Number|Flow Direction|Class Weight|Traffic Count|\n",
            "+----+-----+---+----+-------+-----------+--------------+------------+-------------+\n",
            "|0   |0    |0  |0   |0      |0          |0             |0           |0            |\n",
            "+----+-----+---+----+-------+-----------+--------------+------------+-------------+\n",
            "\n",
            "\n",
            "Temporal feature ranges:\n",
            "  Year range: 2017 to 2025\n",
            "  Month range: 1 to 12\n",
            "  Hour values: [0, 12]\n",
            "\n",
            "Flow Direction category distribution:\n",
            "+--------------+------+\n",
            "|Flow Direction| count|\n",
            "+--------------+------+\n",
            "|             1|649176|\n",
            "|             2|643475|\n",
            "|         Other|630083|\n",
            "+--------------+------+\n",
            "\n",
            "Class Weight distribution:\n",
            "+------------+-------+\n",
            "|Class Weight|  count|\n",
            "+------------+-------+\n",
            "|       Light|1216337|\n",
            "|       Heavy| 706397|\n",
            "+------------+-------+\n",
            "\n",
            "Weekday distribution:\n",
            "+-------+------+\n",
            "|weekday| count|\n",
            "+-------+------+\n",
            "|    Fri|274946|\n",
            "|    Mon|273716|\n",
            "|    Sat|275395|\n",
            "|    Sun|273326|\n",
            "|    Thu|275152|\n",
            "|    Tue|274455|\n",
            "|    Wed|275744|\n",
            "+-------+------+\n",
            "\n",
            "\n",
            "Sample of constructed data with new features:\n",
            "+----+-----+---+----+-------+-----------+--------------+------------+-------------+\n",
            "|year|month|day|hour|weekday|Lane Number|Flow Direction|Class Weight|Traffic Count|\n",
            "+----+-----+---+----+-------+-----------+--------------+------------+-------------+\n",
            "|2018|4    |5  |0   |Thu    |2          |2             |Light       |18585.0      |\n",
            "|2018|4    |5  |0   |Thu    |1          |1             |Heavy       |924.0        |\n",
            "|2018|4    |5  |0   |Thu    |1          |1             |Light       |18508.0      |\n",
            "|2018|4    |5  |0   |Thu    |2          |2             |Heavy       |930.0        |\n",
            "|2018|4    |6  |0   |Fri    |2          |2             |Heavy       |938.0        |\n",
            "+----+-----+---+----+-------+-----------+--------------+------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "\n",
            "Construction Summary:\n",
            "  - Original analytical features: 3\n",
            "  - Newly constructed features: 6\n",
            "  - Total analytical features: 9\n",
            "  - Records maintained: 1,922,734\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample of data:\")\n",
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5DgrRexmgYD",
        "outputId": "db7905fe-ceac-423e-a9d1-d2f271c9bbe5"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of data:\n",
            "+--------------------+-------------+------------+-------------+-----------+-----------------+-------------------+----+-----+---+----+-------+-------+--------------+\n",
            "|Start Date          |Region Name  |Class Weight|Traffic Count|Lane Number|traffic_count_raw|StartDate_parsed   |year|month|day|hour|dow_num|weekday|Flow Direction|\n",
            "+--------------------+-------------+------------+-------------+-----------+-----------------+-------------------+----+-----+---+----+-------+-------+--------------+\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|Light       |18585.0      |2          |18585.0          |2018-04-05 00:00:00|2018|4    |5  |0   |5      |Thu    |2             |\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|Heavy       |924.0        |1          |924.0            |2018-04-05 00:00:00|2018|4    |5  |0   |5      |Thu    |1             |\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|Light       |18508.0      |1          |18508.0          |2018-04-05 00:00:00|2018|4    |5  |0   |5      |Thu    |1             |\n",
            "|4/5/2018 12:00:00 AM|02 - Auckland|Heavy       |930.0        |2          |930.0            |2018-04-05 00:00:00|2018|4    |5  |0   |5      |Thu    |2             |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|Heavy       |938.0        |2          |938.0            |2018-04-06 00:00:00|2018|4    |6  |0   |6      |Fri    |2             |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|Light       |18775.5      |1          |18775.5          |2018-04-06 00:00:00|2018|4    |6  |0   |6      |Fri    |1             |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|Light       |18914.0      |2          |18914.0          |2018-04-06 00:00:00|2018|4    |6  |0   |6      |Fri    |2             |\n",
            "|4/6/2018 12:00:00 AM|02 - Auckland|Heavy       |943.5        |1          |943.5            |2018-04-06 00:00:00|2018|4    |6  |0   |6      |Fri    |1             |\n",
            "|4/7/2018 12:00:00 AM|02 - Auckland|Heavy       |571.0        |2          |571.0            |2018-04-07 00:00:00|2018|4    |7  |0   |7      |Sat    |2             |\n",
            "|4/7/2018 12:00:00 AM|02 - Auckland|Light       |17523.0      |2          |17523.0          |2018-04-07 00:00:00|2018|4    |7  |0   |7      |Sat    |2             |\n",
            "+--------------------+-------------+------------+-------------+-----------+-----------------+-------------------+----+-----+---+----+-------+-------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, lit, to_date, year, month, dayofmonth\n",
        "\n",
        "# 1) Load and normalize holiday file\n",
        "hol = spark.read.csv(holidays_fp, header=True, inferSchema=True)\n",
        "print(f\"[INFO] Holiday file columns: {hol.columns}\")\n",
        "\n",
        "# lowercase all column names to avoid case mismatches\n",
        "for c in hol.columns:\n",
        "    hol = hol.withColumnRenamed(c, c.lower())\n",
        "\n",
        "if \"date\" not in hol.columns:\n",
        "    raise ValueError(\"NZ_Holidays.csv missing 'date' column\")\n",
        "\n",
        "# 2) Build join keys and a single holiday flag column on the RIGHT side\n",
        "hol = (\n",
        "    hol.withColumn(\"date_only\", to_date(col(\"date\")))\n",
        "       .withColumn(\"year\",  year(col(\"date_only\")))\n",
        "       .withColumn(\"month\", month(col(\"date_only\")))\n",
        "       .withColumn(\"day\",   dayofmonth(col(\"date_only\")))\n",
        "       .select(\"year\", \"month\", \"day\")\n",
        "       .distinct()\n",
        "       .withColumn(\"hol_is_holiday\", lit(True))  # right-side flag with unique name\n",
        ")\n",
        "\n",
        "print(f\"[INFO] Holiday unique dates: {hol.count()}\")\n",
        "\n",
        "# 3) Ensure main df has join keys\n",
        "join_keys = [\"year\", \"month\", \"day\"]\n",
        "missing_keys = [k for k in join_keys if k not in df.columns]\n",
        "if missing_keys:\n",
        "    raise ValueError(f\"Missing join keys in main df: {missing_keys}. Make sure 3.3 created year/month/day.\")\n",
        "\n",
        "# 4) Drop any existing is_holiday in main df to avoid ambiguity (safe/idempotent)\n",
        "if \"is_holiday\" in df.columns:\n",
        "    df = df.drop(\"is_holiday\")\n",
        "\n",
        "# 5) Alias, left join, coalesce the right-side flag into a single 'is_holiday'\n",
        "l = df.alias(\"l\")\n",
        "r = hol.alias(\"r\")\n",
        "df = (\n",
        "    l.join(r, on=join_keys, how=\"left\")\n",
        "     .withColumn(\"is_holiday\", F.coalesce(col(\"r.hol_is_holiday\"), lit(False)))\n",
        "     .drop(\"hol_is_holiday\")  # drop the right-side helper column\n",
        ")\n",
        "\n",
        "# 6) Optional: cast to int for modelling compatibility\n",
        "df = df.withColumn(\"is_holiday\", col(\"is_holiday\").cast(\"int\"))\n",
        "\n",
        "# 7) Sanity checks\n",
        "print(f\"[DEBUG] Final df columns: {df.columns}\")\n",
        "holiday_count = df.filter(col(\"is_holiday\") == 1).count()\n",
        "print(f\"[INFO] Records marked as holidays: {holiday_count:,}\")\n",
        "null_report(df.select([c for c in [\"year\",\"month\",\"day\",\"is_holiday\"] if c in df.columns]), \"after holiday join\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TKra5Pt5aMP8",
        "outputId": "7689d1ec-54a5-4106-db37-b42502c38433"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Holiday file columns: ['date', 'name']\n",
            "[INFO] Holiday unique dates: 103\n",
            "[DEBUG] Final df columns: ['year', 'month', 'day', 'Start Date', 'Region Name', 'Class Weight', 'Traffic Count', 'Lane Number', 'traffic_count_raw', 'StartDate_parsed', 'hour', 'dow_num', 'weekday', 'Flow Direction', 'is_holiday']\n",
            "[INFO] Records marked as holidays: 60,557\n",
            "[NULL REPORT] after holiday join rows=1922734 cols=4\n",
            "+----+-----+---+----------+\n",
            "|year|month|day|is_holiday|\n",
            "+----+-----+---+----------+\n",
            "|0   |0    |0  |0         |\n",
            "+----+-----+---+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample of data:\")\n",
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XGimqQXmjS-",
        "outputId": "2539bae9-fb89-431b-b522-19f5b2982a69"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of data:\n",
            "+----+-----+---+--------------------+-------------+------------+-------------+-----------+-----------------+-------------------+----+-------+-------+--------------+----------+\n",
            "|year|month|day|Start Date          |Region Name  |Class Weight|Traffic Count|Lane Number|traffic_count_raw|StartDate_parsed   |hour|dow_num|weekday|Flow Direction|is_holiday|\n",
            "+----+-----+---+--------------------+-------------+------------+-------------+-----------+-----------------+-------------------+----+-------+-------+--------------+----------+\n",
            "|2018|4    |5  |4/5/2018 12:00:00 AM|02 - Auckland|Light       |18585.0      |2          |18585.0          |2018-04-05 00:00:00|0   |5      |Thu    |2             |0         |\n",
            "|2018|4    |5  |4/5/2018 12:00:00 AM|02 - Auckland|Heavy       |924.0        |1          |924.0            |2018-04-05 00:00:00|0   |5      |Thu    |1             |0         |\n",
            "|2018|4    |5  |4/5/2018 12:00:00 AM|02 - Auckland|Light       |18508.0      |1          |18508.0          |2018-04-05 00:00:00|0   |5      |Thu    |1             |0         |\n",
            "|2018|4    |5  |4/5/2018 12:00:00 AM|02 - Auckland|Heavy       |930.0        |2          |930.0            |2018-04-05 00:00:00|0   |5      |Thu    |2             |0         |\n",
            "|2018|4    |6  |4/6/2018 12:00:00 AM|02 - Auckland|Heavy       |938.0        |2          |938.0            |2018-04-06 00:00:00|0   |6      |Fri    |2             |0         |\n",
            "|2018|4    |6  |4/6/2018 12:00:00 AM|02 - Auckland|Light       |18775.5      |1          |18775.5          |2018-04-06 00:00:00|0   |6      |Fri    |1             |0         |\n",
            "|2018|4    |6  |4/6/2018 12:00:00 AM|02 - Auckland|Light       |18914.0      |2          |18914.0          |2018-04-06 00:00:00|0   |6      |Fri    |2             |0         |\n",
            "|2018|4    |6  |4/6/2018 12:00:00 AM|02 - Auckland|Heavy       |943.5        |1          |943.5            |2018-04-06 00:00:00|0   |6      |Fri    |1             |0         |\n",
            "|2018|4    |7  |4/7/2018 12:00:00 AM|02 - Auckland|Heavy       |571.0        |2          |571.0            |2018-04-07 00:00:00|0   |7      |Sat    |2             |0         |\n",
            "|2018|4    |7  |4/7/2018 12:00:00 AM|02 - Auckland|Light       |17523.0      |2          |17523.0          |2018-04-07 00:00:00|0   |7      |Sat    |2             |0         |\n",
            "+----+-----+---+--------------------+-------------+------------+-------------+-----------+-----------------+-------------------+----+-------+-------+--------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3.5 Data Reformatting and Export\n",
        "\n",
        "\n",
        "# helper to pick whichever variant exists\n",
        "def pick(*cands):\n",
        "    for c in cands:\n",
        "        if c in df.columns:\n",
        "            return c\n",
        "    return None\n",
        "\n",
        "# Final schema standardization - ensure consistent column selection\n",
        "final_analytical_vars = [\n",
        "    pick(\"year\"),\n",
        "    pick(\"month\"),\n",
        "    pick(\"day\"),\n",
        "    pick(\"hour\"),\n",
        "    pick(\"weekday\"),\n",
        "    pick(\"region_name\", \"Region Name\"),\n",
        "    pick(\"lane_number\", \"Lane Number\"),\n",
        "    pick(\"flow_direction\", \"Flow Direction\"),\n",
        "    pick(\"class_weight\", \"Class Weight\"),\n",
        "    pick(\"is_holiday\"),\n",
        "    pick(\"traffic_count\", \"Traffic Count\")\n",
        "]\n",
        "\n",
        "# Add audit trail if available\n",
        "raw_target = pick(\"traffic_count_raw\")\n",
        "if raw_target:\n",
        "    final_analytical_vars.append(raw_target)\n",
        "\n",
        "# Select final modeling dataset\n",
        "final_cols = [c for c in final_analytical_vars if c]\n",
        "df_final = df.select(*final_cols)\n",
        "\n",
        "print(f\"[SCHEMA] Final dataset structure: {len(final_cols)} variables selected\")\n",
        "print(f\"[SCHEMA] Variables: {[c for c in final_cols]}\")\n",
        "\n",
        "# Final data export in multiple formats\n",
        "out_parquet = \"/content/drive/MyDrive/722/output/03_prepared.parquet\"\n",
        "out_csv = \"/content/drive/MyDrive/722/output/03_prepared.csv\"\n",
        "\n",
        "print(f\"[EXPORT] Saving to Parquet format...\")\n",
        "df_final.repartition(1).write.mode(\"overwrite\").parquet(out_parquet)\n",
        "\n",
        "print(f\"[EXPORT] Saving to CSV format...\")\n",
        "df_final.repartition(1).write.mode(\"overwrite\").option(\"header\", True).csv(out_csv)\n",
        "\n",
        "print(f\"[SAVED] {out_parquet}\")\n",
        "print(f\"[SAVED] {out_csv}\")\n",
        "\n",
        "# Final dataset characterization\n",
        "count_log(df_final, \"prepared dataset\")\n",
        "print(f\"[FINAL] Ready for modeling: {df_final.count():,} records, {len(df_final.columns)} variables\")\n",
        "\n",
        "# 3.6 Dataset Summary & Validation\n",
        "\n",
        "# resolve column names for summary\n",
        "target_c  = pick(\"traffic_count\", \"Traffic Count\")\n",
        "flow_c    = pick(\"flow_direction\", \"Flow Direction\")\n",
        "class_c   = pick(\"class_weight\", \"Class Weight\")\n",
        "weekday_c = pick(\"weekday\")\n",
        "holiday_c = pick(\"is_holiday\")\n",
        "raw_c     = pick(\"traffic_count_raw\")\n",
        "\n",
        "# Categorical variable distributions\n",
        "print(\"\\n[CATEGORICAL DISTRIBUTIONS]\")\n",
        "for var_name, col_name in [(\"Flow Direction\", flow_c), (\"Class Weight\", class_c), (\"Weekday\", weekday_c), (\"Holiday Status\", holiday_c)]:\n",
        "    if col_name:\n",
        "        print(f\"\\n{var_name}:\")\n",
        "        df_final.groupBy(col_name).count().orderBy(F.desc(\"count\")).show(10, truncate=False)\n",
        "\n",
        "# Target variable descriptive statistics\n",
        "if target_c:\n",
        "    print(f\"\\n[TARGET VARIABLE STATISTICS] - {target_c}\")\n",
        "    stats_df = df_final.select(\n",
        "        F.mean(col(target_c)).alias(\"mean\"),\n",
        "        F.percentile_approx(col(target_c), 0.5).alias(\"median\"),\n",
        "        F.min(col(target_c)).alias(\"min\"),\n",
        "        F.max(col(target_c)).alias(\"max\"),\n",
        "        F.stddev(col(target_c)).alias(\"std\")\n",
        "    )\n",
        "    stats_df.show(truncate=False)\n",
        "\n",
        "# Data preparation impact summary\n",
        "print(\"\\n[DATA PREPARATION IMPACT SUMMARY]\")\n",
        "\n",
        "if holiday_c:\n",
        "    holiday_pct = df_final.agg(F.mean(col(holiday_c).cast(\"double\"))).collect()[0][0]\n",
        "    print(f\"  Holiday coverage: {holiday_pct:.2%} of records\")\n",
        "\n",
        "if raw_c and target_c:\n",
        "    capped_pct = df_final.filter(col(raw_c) != col(target_c)).count() / df_final.count()\n",
        "    print(f\"  Traffic count capping applied: {capped_pct:.4%} of records\")\n",
        "\n",
        "print(f\"  Final record count: {df_final.count():,}\")\n",
        "print(f\"  Final variable count: {len(df_final.columns)}\")\n",
        "print(f\"  Data completeness: 100% (all preparation stages successful)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcQUtBXkaRrE",
        "outputId": "c25a9fb8-5884-4f4f-f996-4af9aa69e2ff"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[SCHEMA] Final dataset structure: 12 variables selected\n",
            "[SCHEMA] Variables: ['year', 'month', 'day', 'hour', 'weekday', 'Region Name', 'Lane Number', 'Flow Direction', 'Class Weight', 'is_holiday', 'Traffic Count', 'traffic_count_raw']\n",
            "[EXPORT] Saving to Parquet format...\n",
            "[EXPORT] Saving to CSV format...\n",
            "[SAVED] /content/drive/MyDrive/722/output/03_prepared.parquet\n",
            "[SAVED] /content/drive/MyDrive/722/output/03_prepared.csv\n",
            "[COUNT] prepared dataset -> 1922734\n",
            "[FINAL] Ready for modeling: 1,922,734 records, 12 variables\n",
            "\n",
            "[CATEGORICAL DISTRIBUTIONS]\n",
            "\n",
            "Flow Direction:\n",
            "+--------------+------+\n",
            "|Flow Direction|count |\n",
            "+--------------+------+\n",
            "|1             |649176|\n",
            "|2             |643475|\n",
            "|Other         |630083|\n",
            "+--------------+------+\n",
            "\n",
            "\n",
            "Class Weight:\n",
            "+------------+-------+\n",
            "|Class Weight|count  |\n",
            "+------------+-------+\n",
            "|Light       |1216337|\n",
            "|Heavy       |706397 |\n",
            "+------------+-------+\n",
            "\n",
            "\n",
            "Weekday:\n",
            "+-------+------+\n",
            "|weekday|count |\n",
            "+-------+------+\n",
            "|Wed    |275744|\n",
            "|Sat    |275395|\n",
            "|Thu    |275152|\n",
            "|Fri    |274946|\n",
            "|Tue    |274455|\n",
            "|Mon    |273716|\n",
            "|Sun    |273326|\n",
            "+-------+------+\n",
            "\n",
            "\n",
            "Holiday Status:\n",
            "+----------+-------+\n",
            "|is_holiday|count  |\n",
            "+----------+-------+\n",
            "|0         |1862177|\n",
            "|1         |60557  |\n",
            "+----------+-------+\n",
            "\n",
            "\n",
            "[TARGET VARIABLE STATISTICS] - Traffic Count\n",
            "+------------------+------+---+-------+-----------------+\n",
            "|mean              |median|min|max    |std              |\n",
            "+------------------+------+---+-------+-----------------+\n",
            "|7320.2989363063225|5681.5|0.0|33898.5|6983.130776471687|\n",
            "+------------------+------+---+-------+-----------------+\n",
            "\n",
            "\n",
            "[DATA PREPARATION IMPACT SUMMARY]\n",
            "  Holiday coverage: 3.15% of records\n",
            "  Traffic count capping applied: 0.0000% of records\n",
            "  Final record count: 1,922,734\n",
            "  Final variable count: 12\n",
            "  Data completeness: 100% (all preparation stages successful)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sample of data:\")\n",
        "df.show(10, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLAeDTY51Cu1",
        "outputId": "9da86a52-7f87-4593-c0d2-59d3917794b0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample of data:\n",
            "+----+-----+---+-------------+------------+-------------+-----------+-----------------+----+-------+-------+--------------+----------+\n",
            "|year|month|day|Region Name  |Class Weight|Traffic Count|Lane Number|traffic_count_raw|hour|dow_num|weekday|Flow Direction|is_holiday|\n",
            "+----+-----+---+-------------+------------+-------------+-----------+-----------------+----+-------+-------+--------------+----------+\n",
            "|2018|4    |5  |02 - Auckland|Light       |18585.0      |2          |18585.0          |0   |5      |Thu    |2             |0         |\n",
            "|2018|4    |5  |02 - Auckland|Heavy       |924.0        |1          |924.0            |0   |5      |Thu    |1             |0         |\n",
            "|2018|4    |5  |02 - Auckland|Light       |18508.0      |1          |18508.0          |0   |5      |Thu    |1             |0         |\n",
            "|2018|4    |5  |02 - Auckland|Heavy       |930.0        |2          |930.0            |0   |5      |Thu    |2             |0         |\n",
            "|2018|4    |6  |02 - Auckland|Heavy       |938.0        |2          |938.0            |0   |6      |Fri    |2             |0         |\n",
            "|2018|4    |6  |02 - Auckland|Light       |18775.5      |1          |18775.5          |0   |6      |Fri    |1             |0         |\n",
            "|2018|4    |6  |02 - Auckland|Light       |18914.0      |2          |18914.0          |0   |6      |Fri    |2             |0         |\n",
            "|2018|4    |6  |02 - Auckland|Heavy       |943.5        |1          |943.5            |0   |6      |Fri    |1             |0         |\n",
            "|2018|4    |7  |02 - Auckland|Heavy       |571.0        |2          |571.0            |0   |7      |Sat    |2             |0         |\n",
            "|2018|4    |7  |02 - Auckland|Light       |17523.0      |2          |17523.0          |0   |7      |Sat    |2             |0         |\n",
            "+----+-----+---+-------------+------------+-------------+-----------+-----------------+----+-------+-------+--------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}